import os
import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)

# === C·∫•u h√¨nh ===
PHOBERT_PATH = "./phobert-emotion-final"
QWEN_MODEL_NAME = "Qwen/Qwen1.5-1.8B-Chat"

# === T·∫£i PhoBERT ===
print("üîÑ ƒêang t·∫£i model ph√¢n t√≠ch c·∫£m x√∫c (PhoBERT)...")
phobert_model = AutoModelForSequenceClassification.from_pretrained(PHOBERT_PATH)
phobert_tokenizer = AutoTokenizer.from_pretrained(PHOBERT_PATH)
id2label = phobert_model.config.id2label

# === T·∫£i Qwen ===
print("üîÑ ƒêang t·∫£i LLM Qwen (l·∫ßn ƒë·∫ßu c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME, trust_remote_code=True)
qwen_model = AutoModelForCausalLM.from_pretrained(
    QWEN_MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True
)

qwen_pipe = pipeline(
    "text-generation",
    model=qwen_model,
    tokenizer=qwen_tokenizer,
    device_map="auto"
)

# === H√†m h·ªó tr·ª£ ===
def get_emotion(text):
    inputs = phobert_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    ).to(phobert_model.device)
    with torch.no_grad():
        logits = phobert_model(**inputs).logits
        pred_id = logits.argmax().item()
    return id2label[pred_id]

def generate_response(emotion, user_text, max_new_tokens=128):
    prompt = f"""B·∫°n l√† m·ªôt ng∆∞·ªùi b·∫°n ƒë·ªìng c·∫£m v√† nh·∫π nh√†ng. 
H√£y ph·∫£n h·ªìi ng·∫Øn g·ªçn (1-2 c√¢u), ch√¢n th√†nh v√† an ·ªßi ng∆∞·ªùi ƒëang c·∫£m th·∫•y "{emotion}".
Kh√¥ng c·∫ßn l·∫∑p l·∫°i c·∫£m x√∫c, ch·ªâ c·∫ßn th·ªÉ hi·ªán s·ª± th·∫•u hi·ªÉu v√† ƒë·ªông vi√™n.

L·ªùi t√¢m s·ª±: "{user_text}"

Ph·∫£n h·ªìi:"""
    
    response = qwen_pipe(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=qwen_tokenizer.eos_token_id
    )
    
    full_text = response[0]['generated_text']
    reply = full_text.split("Ph·∫£n h·ªìi:")[-1].strip()
    return reply

# === V√≤ng l·∫∑p chat ===
if __name__ == "__main__":
    print("üí¨ Xin ch√†o! M√¨nh l√† ng∆∞·ªùi b·∫°n ƒë·ªìng c·∫£m. B·∫°n c√≥ th·ªÉ chia s·∫ª b·∫•t k·ª≥ ƒëi·ªÅu g√¨.")
    print("G√µ 'exit' ƒë·ªÉ d·ª´ng.\n")

    while True:
        try:
            user_input = input("B·∫°n: ").strip()
            if user_input.lower() in ['exit', 'quit', 'tho√°t']:
                print("\nT·∫°m bi·ªát! ‚ù§Ô∏è")
                break
            if not user_input:
                continue

            emotion = get_emotion(user_input)
            reply = generate_response(emotion, user_input)

            print(f"\nüß† C·∫£m x√∫c: {emotion}")
            print(f"üí¨ Qwen: {reply}\n")

        except KeyboardInterrupt:
            print("\n\nT·∫°m bi·ªát! ‚ù§Ô∏è")
            break
        except Exception as e:
            print(f"\n‚ùå L·ªói: {e}")
            print("Vui l√≤ng th·ª≠ l·∫°i.\n")